#!/usr/bin/env python3
"""
æ€§èƒ½å¯¹æ¯”æµ‹è¯•
æ¯”è¾ƒåŒæ­¥ã€å¤šçº¿ç¨‹ã€å¼‚æ­¥çš„æ€§èƒ½å·®å¼‚
è¿è¡Œæ–¹æ³•ï¼špython performance_test.py
"""

import asyncio
import aiohttp
import requests
import time
import concurrent.futures
import multiprocessing
from typing import List, Dict, Callable
import statistics
import json

# æµ‹è¯•URLåˆ—è¡¨ - ä½¿ç”¨ç¨³å®šçš„æµ‹è¯•URL
TEST_URLS = [
    'https://httpbin.org/delay/1',
    'https://jsonplaceholder.typicode.com/posts/1',
    'https://httpbin.org/delay/2',
    'https://jsonplaceholder.typicode.com/posts/2',
    'https://httpbin.org/user-agent',
    'https://jsonplaceholder.typicode.com/posts/3',
    'https://httpbin.org/headers',
    'https://jsonplaceholder.typicode.com/posts/4',
    'https://httpbin.org/delay/1',
    'https://jsonplaceholder.typicode.com/posts/5',
]

class PerformanceTester:
    """æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.results = {}
    
    def sync_download(self, urls: List[str]) -> List[str]:
        """åŒæ­¥ä¸‹è½½"""
        results = []
        for url in urls:
            try:
                response = requests.get(url, timeout=10)
                results.append(f"{url}: {len(response.text)} chars")
            except Exception as e:
                results.append(f"{url}: ERROR - {e}")
        return results
    
    def thread_download(self, urls: List[str], max_workers: int = 5) -> List[str]:
        """å¤šçº¿ç¨‹ä¸‹è½½"""
        def download_one(url):
            try:
                response = requests.get(url, timeout=10)
                return f"{url}: {len(response.text)} chars"
            except Exception as e:
                return f"{url}: ERROR - {e}"
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = list(executor.map(download_one, urls))
        return results
    
    def process_download(self, urls: List[str]) -> List[str]:
        """å¤šè¿›ç¨‹ä¸‹è½½"""
        def download_one(url):
            try:
                response = requests.get(url, timeout=10)
                return f"{url}: {len(response.text)} chars"
            except Exception as e:
                return f"{url}: ERROR - {e}"
        
        with multiprocessing.Pool(processes=min(len(urls), multiprocessing.cpu_count())) as pool:
            results = pool.map(download_one, urls)
        return results
    
    async def async_download(self, urls: List[str], max_concurrent: int = 5) -> List[str]:
        """å¼‚æ­¥ä¸‹è½½"""
        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=10),
            connector=aiohttp.TCPConnector(limit=100, limit_per_host=10)
        ) as session:
            
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def download_one(url):
                async with semaphore:
                    try:
                        async with session.get(url) as response:
                            content = await response.text()
                            return f"{url}: {len(content)} chars"
                    except Exception as e:
                        return f"{url}: ERROR - {e}"
            
            results = await asyncio.gather(*[download_one(url) for url in urls])
            return results
    
    def measure_performance(self, func: Callable, *args, **kwargs) -> Dict[str, float]:
        """æµ‹é‡å‡½æ•°æ€§èƒ½"""
        times = []
        
        # é¢„çƒ­
        try:
            if asyncio.iscoroutinefunction(func):
                asyncio.run(func(*args, **kwargs))
            else:
                func(*args, **kwargs)
        except:
            pass
        
        # å¤šæ¬¡æµ‹è¯•å–å¹³å‡å€¼
        num_runs = 3
        for i in range(num_runs):
            start_time = time.time()
            
            try:
                if asyncio.iscoroutinefunction(func):
                    result = asyncio.run(func(*args, **kwargs))
                else:
                    result = func(*args, **kwargs)
                
                elapsed = time.time() - start_time
                times.append(elapsed)
                print(f"  è¿è¡Œ {i+1}: {elapsed:.2f}ç§’")
                
            except Exception as e:
                print(f"  è¿è¡Œ {i+1} å¤±è´¥: {e}")
                times.append(float('inf'))
        
        if times and all(t != float('inf') for t in times):
            return {
                'min_time': min(times),
                'max_time': max(times),
                'avg_time': statistics.mean(times),
                'median_time': statistics.median(times),
                'std_dev': statistics.stdev(times) if len(times) > 1 else 0
            }
        else:
            return {'error': 'æµ‹è¯•å¤±è´¥'}
    
    def run_comparison(self, urls: List[str]) -> Dict[str, Dict]:
        """è¿è¡Œå®Œæ•´æ€§èƒ½å¯¹æ¯”"""
        print("ğŸš€ æ€§èƒ½å¯¹æ¯”æµ‹è¯•å¼€å§‹")
        print("=" * 60)
        print(f"æµ‹è¯•URLæ•°é‡: {len(urls)}")
        print(f"æµ‹è¯•URL: {urls[:3]}...")
        print("=" * 60)
        
        test_cases = {
            'åŒæ­¥ä¸‹è½½': lambda: self.sync_download(urls),
            'å¤šçº¿ç¨‹(3 workers)': lambda: self.thread_download(urls, 3),
            'å¤šçº¿ç¨‹(5 workers)': lambda: self.thread_download(urls, 5),
            'å¤šçº¿ç¨‹(10 workers)': lambda: self.thread_download(urls, 10),
            'å¤šè¿›ç¨‹': lambda: self.process_download(urls),
            'å¼‚æ­¥(3å¹¶å‘)': lambda: self.async_download(urls, 3),
            'å¼‚æ­¥(5å¹¶å‘)': lambda: self.async_download(urls, 5),
            'å¼‚æ­¥(10å¹¶å‘)': lambda: self.async_download(urls, 10),
        }
        
        self.results = {}
        
        for test_name, test_func in test_cases.items():
            print(f"\nğŸ“Š æµ‹è¯•: {test_name}")
            print("-" * 40)
            
            perf_result = self.measure_performance(test_func)
            self.results[test_name] = perf_result
            
            if 'error' not in perf_result:
                print(f"  å¹³å‡: {perf_result['avg_time']:.2f}ç§’")
                print(f"  æœ€å¿«: {perf_result['min_time']:.2f}ç§’")
                print(f"  æœ€æ…¢: {perf_result['max_time']:.2f}ç§’")
                print(f"  æ ‡å‡†å·®: {perf_result['std_dev']:.2f}ç§’")
            else:
                print(f"  é”™è¯¯: {perf_result['error']}")
        
        return self.results
    
    def generate_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        if not self.results:
            return "æ²¡æœ‰æµ‹è¯•ç»“æœ"
        
        # è¿‡æ»¤æ‰å¤±è´¥çš„æµ‹è¯•
        valid_results = {k: v for k, v in self.results.items() 
                        if 'error' not in v}
        
        if not valid_results:
            return "æ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥äº†"
        
        # æŒ‰å¹³å‡æ—¶é—´æ’åº
        sorted_results = sorted(valid_results.items(), 
                              key=lambda x: x[1]['avg_time'])
        
        # è®¡ç®—æ•ˆç‡å€æ•°
        sync_time = valid_results.get('åŒæ­¥ä¸‹è½½', {}).get('avg_time', 1)
        
        report = []
        report.append("\n" + "=" * 60)
        report.append("ğŸ“ˆ æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 60)
        
        for rank, (test_name, result) in enumerate(sorted_results, 1):
            speedup = sync_time / result['avg_time'] if sync_time > 0 else 1
            report.append(f"{rank}. {test_name}")
            report.append(f"   å¹³å‡æ—¶é—´: {result['avg_time']:.2f}ç§’")
            report.append(f"   ç›¸å¯¹åŒæ­¥: {speedup:.1f}x æ›´å¿«")
            report.append("-" * 40)
        
        # æ‰¾å‡ºæœ€ä½³é…ç½®
        best_method = sorted_results[0]
        report.append(f"\nğŸ† æœ€ä½³æ–¹æ³•: {best_method[0]}")
        report.append(f"   æ¯”åŒæ­¥å¿« {sync_time/best_method[1]["avg_time"]:.1f} å€")
        
        return "\n".join(report)
    
    def save_results(self, filename: str = 'performance_results.json'):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")

class ScalabilityTester:
    """å¯æ‰©å±•æ€§æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.scalability_results = {}
    
    async def test_concurrency_levels(self, base_urls: List[str]) -> Dict[int, float]:
        """æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«çš„æ€§èƒ½"""
        print("\nğŸ” å¹¶å‘çº§åˆ«å¯æ‰©å±•æ€§æµ‹è¯•")
        print("-" * 40)
        
        levels = [1, 2, 3, 5, 8, 10, 15, 20]
        results = {}
        
        for level in levels:
            print(f"æµ‹è¯•å¹¶å‘çº§åˆ«: {level}")
            
            async def test_level():
                start_time = time.time()
                await AsyncDownloader().async_download(base_urls, level)
                return time.time() - start_time
            
            elapsed = await test_level()
            results[level] = elapsed
            print(f"  è€—æ—¶: {elapsed:.2f}ç§’")
        
        return results
    
    def analyze_optimal_concurrency(self, results: Dict[int, float]) -> int:
        """åˆ†ææœ€ä½³å¹¶å‘æ•°"""
        if not results:
            return 5  # é»˜è®¤å€¼
        
        # æ‰¾å‡ºæ€§èƒ½æå‡å¼€å§‹é€’å‡çš„ç‚¹
        sorted_levels = sorted(results.items())
        
        best_level = 1
        best_efficiency = float('inf')
        
        for level, time_taken in sorted_levels:
            efficiency = time_taken / level  # æ¯ä¸ªä»»åŠ¡çš„å¹³å‡æ—¶é—´
            if efficiency < best_efficiency:
                best_efficiency = efficiency
                best_level = level
        
        return best_level

class IOTaskSimulator:
    """I/Oä»»åŠ¡æ¨¡æ‹Ÿå™¨"""
    
    @staticmethod
    async def simulate_io_task(task_id: int, delay: float = 1.0):
        """æ¨¡æ‹ŸI/Oå¯†é›†å‹ä»»åŠ¡"""
        await asyncio.sleep(delay + random.uniform(-0.2, 0.2))
        return f"ä»»åŠ¡{task_id}å®Œæˆ"
    
    @staticmethod
    def simulate_cpu_task(task_id: int, iterations: int = 1000000):
        """æ¨¡æ‹ŸCPUå¯†é›†å‹ä»»åŠ¡"""
        result = sum(i * i for i in range(iterations))
        return f"ä»»åŠ¡{task_id}å®Œæˆï¼Œç»“æœ: {result}"

async def run_io_vs_cpu_comparison():
    """è¿è¡ŒI/O vs CPUå¯†é›†å‹ä»»åŠ¡å¯¹æ¯”"""
    print("\nâš¡ I/O vs CPUå¯†é›†å‹ä»»åŠ¡å¯¹æ¯”")
    print("-" * 40)
    
    # I/Oå¯†é›†å‹æµ‹è¯•
    io_tasks = [IOTaskSimulator.simulate_io_task(i, 0.5) for i in range(10)]
    start_time = time.time()
    io_results = await asyncio.gather(*io_tasks)
    io_time = time.time() - start_time
    
    print(f"å¼‚æ­¥I/Oä»»åŠ¡: {io_time:.2f}ç§’ (10ä¸ªä»»åŠ¡)")
    
    # CPUå¯†é›†å‹æµ‹è¯• - å¤šè¿›ç¨‹
    start_time = time.time()
    with multiprocessing.Pool(processes=4) as pool:
        cpu_results = pool.map(IOTaskSimulator.simulate_cpu_task, range(10))
    cpu_time = time.time() - start_time
    
    print(f"å¤šè¿›ç¨‹CPUä»»åŠ¡: {cpu_time:.2f}ç§’ (10ä¸ªä»»åŠ¡)")
    print(f"I/Oä»»åŠ¡æ•ˆç‡æå‡: {cpu_time/io_time:.1f}x")

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Pythonå¼‚æ­¥æ€§èƒ½æµ‹è¯•å¥—ä»¶")
    print("=" * 60)
    
    # åŸºæœ¬æ€§èƒ½å¯¹æ¯”
    tester = PerformanceTester()
    results = tester.run_comparison(TEST_URLS)
    
    # æ˜¾ç¤ºæŠ¥å‘Š
    report = tester.generate_report()
    print(report)
    
    # ä¿å­˜ç»“æœ
    tester.save_results()
    
    # å¯æ‰©å±•æ€§æµ‹è¯•
    scalability_tester = ScalabilityTester()
    scalability_results = await scalability_tester.test_concurrency_levels(TEST_URLS[:5])
    
    optimal = scalability_tester.analyze_optimal_concurrency(scalability_results)
    print(f"\nğŸ¯ æ¨èå¹¶å‘æ•°: {optimal}")
    
    # I/O vs CPUå¯¹æ¯”
    await run_io_vs_cpu_comparison()
    
    print("\n" + "=" * 60)
    print("âœ… æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
    print("ğŸ“Š æ£€æŸ¥ performance_results.json æŸ¥çœ‹è¯¦ç»†ç»“æœ")

if __name__ == "__main__":
    asyncio.run(main())